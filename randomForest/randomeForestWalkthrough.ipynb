{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3bc3f4",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7da2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sea\n",
    "\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde7f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the libs\n",
    "pd.set_option(\"display.max_columns\", None)  #makes it display all columns in the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333dffb5",
   "metadata": {},
   "source": [
    "# Algorithm Brief Explanation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f56ca73",
   "metadata": {},
   "source": [
    "Decsion trees tend to be very sensitive to the dataset they were trained on. Meaning that if you take a dataset, train a decsion tree on it, and change it slighly. If you re-train the model, the tree produced may be very different from the one you originally started with. This means the model has high variance and may fail to generalize when given new data.\n",
    "\n",
    "To solve this problem, Random Forest comes in. Random forest is an aggregate algorithm that uses muiltiple decsion trees to produce its calculation. \n",
    "\n",
    "1. Randomly select rows from the data set to create new datasets for each new tree created in the forest. Every dataset will contain the same number of rows as the original one. Using random sampling with replacement. This process is called bootstrapping. Bootstrapping ensures that we are not using the same data for every tree produced. This helps the model be less sensitive to the training data. \n",
    "\n",
    "2. Now each tree will be trained on its own data set independently. Each tree will not use every feature possible for training. Instead, a subset will be randomly selected for each tree only they will be used for training. This random feature selection helps reduce the corrolation between the trees. If you used every feature, then most of your trees will have the same decisions nodes and they will act very similarly. That would increase the variance. Another benifit is that some of the trees will be trained on less important features and so they will give bad predictions, but their will also be some trees that bad predictions in the opposite direction. So they will balance out.\n",
    "\n",
    "3. Then build the trees.\n",
    "\n",
    "4. Now to make the predictions. Given a new data point, run it through each tree and record each prediction. Whatever item has the majority vote is the item the model will report. This process of combining results from muiltiple models is called aggregation.\n",
    "\n",
    "Note: Bootstraping + aggregation = bagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b4362b",
   "metadata": {},
   "source": [
    "What is the ideal size of the feature subset? For classification problems, research has found that values close to the log and sqrt of the total number of features work well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b5e23",
   "metadata": {},
   "source": [
    "So for example: \n",
    "num_features_for_split = sqrt(total_input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7b850",
   "metadata": {},
   "source": [
    "# Implement the algorithm from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e005f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d101b4",
   "metadata": {},
   "source": [
    "Modified split function from tutorial. Modified from decsion tree algorithm for random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff76ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best split point for the data\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0]) - 1)\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc78f40c",
   "metadata": {},
   "source": [
    "### Random forest implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a842227",
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_forest:\n",
    "    def __init__(self):\n",
    "        \n",
    "    #Split a dataset based on an attribute and an attribute value\n",
    "    #TODO: more detail explination of purpose\n",
    "    def test_split(self, index, value, dataset):\n",
    "        left, right = list(), list()\n",
    "        for row in dataset:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "    \n",
    "    def gini_index(self, groups, classes):\n",
    "        # count all samples at split point\n",
    "        n_instances = float(sum([len(group) for group in groups]))\n",
    "        # sum weighted Gini index for each group\n",
    "        gini = 0.0\n",
    "        for group in groups:\n",
    "            size = float(len(group))\n",
    "            #avoid dividing by zero\n",
    "            if size == 0:\n",
    "                continue\n",
    "            score = 0.0\n",
    "            # score the group based on the score for each class\n",
    "            for class_val in classes:\n",
    "                p = [row[-1] for row in group].count(class_val) / size\n",
    "                score += p * p\n",
    "            # weight the group score by its relative size\n",
    "            gini += (1.0 - score) * (size / n_instances) # gini formula\n",
    "        return gini\n",
    "    \n",
    "    # Select the best split point for the data\n",
    "    def get_split(dataset, n_features):\n",
    "        class_values = list(set(row[-1] for row in dataset))\n",
    "        b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "        features = list()\n",
    "        while len(features) < n_features:\n",
    "            index = randrange(len(dataset[0]) - 1)\n",
    "            if index not in features:\n",
    "                features.append(index)\n",
    "        for index in features:\n",
    "            for row in dataset:\n",
    "                groups = test_split(index, row[index], dataset)\n",
    "                gini = gini_index(groups, class_values)\n",
    "                if gini < b_score:\n",
    "                    b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "        return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
    "    \n",
    "    # Create a terminal(leaf) node\n",
    "    def to_terminal(self, group):\n",
    "        outcomes = [row[-1] for row in group]\n",
    "        return max(set(outcomes), key = outcomes.count)\n",
    "\n",
    "    # Create child splits for a node or make terminal\n",
    "    def split(self, node, max_depth, min_size, n_features, depth):\n",
    "        left, right = node['groups']\n",
    "        del(node['groups'])\n",
    "        # check for a no split\n",
    "        if not left or not right:\n",
    "            node['left'] = node['right'] = self.to_terminal(left + right)\n",
    "            return\n",
    "        \n",
    "        # Check for max depth\n",
    "        if depth >= max_depth:\n",
    "            node['left'], node['right'] = self.to_terminal(left), self.to_terminal(right)\n",
    "            return\n",
    "        \n",
    "        # process left child\n",
    "        if len(left) <= min_size:\n",
    "            node['left'] = self.to_terminal(left)\n",
    "        else:\n",
    "            node['left'] = self.get_split(left, n_features)\n",
    "            self.split(node['left'], max_depth, min_size, n_features, depth + 1)\n",
    "            \n",
    "        # process right child\n",
    "        if len(right) <= min_size:\n",
    "            node['right'] = self.to_terminal(right)\n",
    "        else:\n",
    "            node['right'] = self.get_split(right, n_features)\n",
    "            self.split(node['right'], max_depth, min_size, n_features, depth + 1)\n",
    "            \n",
    "    # Build a decsion tree\n",
    "    def build_tree(self, train, max_depth, min_size, n_features):\n",
    "        root = self.get_split(train, n_features)\n",
    "        self.split(root, max_depth, min_size, n_features, 1)\n",
    "        return root\n",
    "    \n",
    "    # Make a prediction with a decision tree\n",
    "    def predict(self, node, row):\n",
    "        if row[node['index']] < node['value']:\n",
    "            return self.predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048fb22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f18970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24d36ebd",
   "metadata": {},
   "source": [
    "# Implement the algorithm from library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b08b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
